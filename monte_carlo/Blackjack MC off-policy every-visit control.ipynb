{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "23a218be",
   "metadata": {},
   "source": [
    "# Problem statement\n",
    "Example 5.1 - Reinforcement Learning: An Introduction, Sutton and Barto, Second Edition - Monte Carlo\n",
    "\n",
    "Blackjack is a card game where the goal is to obtain cards that sum to as near as possible to 21 without going over.  They're playing against a fixed dealer.\n",
    "\n",
    "Face cards (Jack, Queen, King) have point value 10. Aces can either count as 11 or 1, and it's called 'usable' at 11.\n",
    "This game is placed with an infinite deck. The game starts with dealer having one face up and one face down card, while player having two face up cards. The player can request additional cards (hit=1) until they decide to stop (stick=0) or exceed 21 (bust).\n",
    "\n",
    "After the player sticks, the dealer reveals their facedown card, and draws until their sum is 17 or greater.  If the dealer goes bust the player wins.\n",
    "\n",
    "If neither player nor dealer busts, the outcome (win, lose, draw) is decided by whose sum is closer to 21.  The reward for winning is +1, drawing is 0, and losing is -1.\n",
    "\n",
    "The observation of a 3-tuple of: the players current sum (0-31), the dealer's one showing card (1-10 where 1 is ace),\n",
    "and whether or not the player holds a usable ace (0 or 1).\n",
    "\n",
    "In this notebook, we calculate the optimal policy using the off-policy every-visit Monte-Carlo method with importance sampling to play Blackjack card game."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "32762dea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import sys\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "if \"../\" not in sys.path:\n",
    "  sys.path.append(\"../\") \n",
    "\n",
    "from utils import plotting\n",
    "%matplotlib inline\n",
    "matplotlib.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "798d77fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('Blackjack-v1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fb4f0166",
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedy_policy(action_values):\n",
    "    \"\"\"\n",
    "    Creates a greedy policy based on the given action_values\n",
    "    \n",
    "    @action_values: maps from state -> action-values\n",
    "    \n",
    "    Returns:\n",
    "        A function that takes the observation (state) as an argument and returns\n",
    "        the probabilities for each action\n",
    "    \n",
    "    \"\"\"\n",
    "    def policy_fn(state):\n",
    "        actions_prob = np.zeros_like(action_values[state], dtype=float)\n",
    "        best_action = np.argmax(action_values[state])\n",
    "        actions_prob[best_action] = 1.0\n",
    "        return actions_prob\n",
    "    return policy_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "79a8c958",
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_policy(nA):\n",
    "    \"\"\"\n",
    "    Creates a random policy function\n",
    "    \n",
    "    @nA: number of actions in the environment\n",
    "    \n",
    "    Returns:\n",
    "        A function that takes the observation (state) as an argument and returns\n",
    "        the probabilities for each action\n",
    "    \"\"\"\n",
    "    actions_prob = np.ones(nA, dtype=float) / nA\n",
    "    def policy_fn(observation):\n",
    "        return actions_prob\n",
    "    return policy_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9c2314a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mc_control_importance_sampling(env, num_episodes, behavior_policy, discount_factor=1.0):\n",
    "    \"\"\"\n",
    "    Monte Carlo control off-policy with importance sampling algorithm\n",
    "    to find the optimal greedy policy\n",
    "    \n",
    "    @env: OpenAI environment\n",
    "    @num_episodes: number of episodes to sample\n",
    "    @behavior_policy: the behavior to follow while generating episodes\n",
    "    @discount_factor: Gamma discount factor\n",
    "    \n",
    "    Returns:\n",
    "        (action_values, policy) tuple\n",
    "    \"\"\"    \n",
    "    # The final action-value function\n",
    "    # maps state -> (action -> action-value)\n",
    "    action_values = defaultdict(lambda: np.zeros(env.action_space.n))\n",
    "    \n",
    "    # (C notation in the book)\n",
    "    importance_sampling_denominator = defaultdict(lambda: np.zeros(env.action_space.n))\n",
    "    \n",
    "    # The greedy policy to be learnt\n",
    "    target_policy = greedy_policy(action_values)\n",
    "    \n",
    "    for i_episode in range(1, num_episodes + 1):\n",
    "        if i_episode % 1000 == 0:\n",
    "            print(f\"\\rEpisode {i_episode}/{num_episodes}\", end=\"\")\n",
    "            sys.stdout.flush()\n",
    "\n",
    "        # Generate an episode: a tuple of (state, action, reward)\n",
    "        episode = []\n",
    "        state = env.reset()\n",
    "        for t in range(20):\n",
    "            # Sample an action from the behavior policy\n",
    "            probs = behavior_policy(state)\n",
    "            # Pick a random action according to the behavior policy probabilities\n",
    "            action = np.random.choice(np.arange(len(probs)), p=probs)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            episode.append((state, action, reward))\n",
    "            if done:\n",
    "                break\n",
    "            state = next_state\n",
    "\n",
    "        G = 0.0 # Sum of discounted returns\n",
    "        importance_sampling = 1.0 # (W notation in the book)\n",
    "        for t in range(len(episode))[::-1]: # iterate backwards through the episode\n",
    "            state, action, reward = episode[t]\n",
    "            G = discount_factor * G + reward\n",
    "            importance_sampling_denominator[state][action] += importance_sampling\n",
    "            \n",
    "            # Update the action-value function using the incremental update formula (5.7) of the book\n",
    "            action_values[state][action] += (importance_sampling / importance_sampling_denominator[state][action]) * \\\n",
    "                                            (G - action_values[state][action])\n",
    "            \n",
    "            # Break if the action taken by the behavior policy is not the action \n",
    "            # taken by the target policy\n",
    "            if action !=  np.argmax(target_policy(state)):\n",
    "                break\n",
    "            importance_sampling = importance_sampling * 1./behavior_policy(state)[action]\n",
    "    \n",
    "    return action_values, target_policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c4daeeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 321000/500000"
     ]
    }
   ],
   "source": [
    "random_policy = random_policy(env.action_space.n)\n",
    "action_values, policy = mc_control_importance_sampling(env, num_episodes=500000, behavior_policy=random_policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eead426",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create state value function from action-value function\n",
    "values = defaultdict(float)\n",
    "for state, actions in action_values.items():\n",
    "    action_value = np.max(actions)\n",
    "    values[state] = action_value\n",
    "plotting.plot_value_function(values, title=\"Optimal Value Function\", minimum_x=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b9c0b7c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
